{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Практикум по программированию на языке Python`\n",
    "<br>\n",
    "\n",
    "## `Занятие 8: Краткий конспект лекции \"Линейные модели для классификации\"`\n",
    "<br><br>\n",
    "\n",
    "### `Находнов Максим (nakhodnov17@gmail.com)`\n",
    "\n",
    "#### `Москва, 2022`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* Определение и мотивация логистической регрессии\n",
    "* Обобщение логистической регрессии на случай многих классов. Мультиномиальная логистическая регрессия\n",
    "* Практические приёмы для численного решения задачи логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Линейная модель бинарной классификации`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Рассмотрим задачу бинарной классификации.\n",
    "Пусть дана обучающая выборка $X = (x_i, y_i)^{l}_{i = 1}$, где $x_i \\in \\mathbb{R}^{d}$, $y_i \\in \\mathbb{Y} = \\{1, -1\\}$. Линейная модель классификации определяется следующим образом:\n",
    "\n",
    "$$\n",
    "a(x) = \\mathrm{sign}\\left( \\langle w, x\\rangle + w_0\\right) \\text{, где $w \\in \\mathbb{R}^{d}$ — вектор весов, $w_0$ — сдвиг. }\n",
    "$$\n",
    "\n",
    "Далее будем считать, что среди признаков есть константа. Тогда классификатор задаётся как:\n",
    "$$a(x) =\\mathrm{sign}\\left( \\langle w, x\\rangle \\right)$$\n",
    "\n",
    "Процесс обучения заключается в настройке вектора $w$.  Величина $M_i(w) = y_i \\langle w, x_i \\rangle$ называется отступом (margin) объекта $x_i$ относительно алгоритма $a(x)$. \n",
    "Если $M_i(w) < 0$, алгоритм допускает ошибку на объекте $x_i$. Чем больше отступ $M_i(w)$, тем более надёжно и правильно алгоритм классифицирует объект $x_i$.\n",
    "\n",
    "Пусть  $\\mathcal{L}(M(w))$ — монотонно невозрастающая функция, такая что $\\mathbb{I}[M(w) < 0] \\leqslant \\mathcal{L}(M(w))$.\n",
    "Будем настраивать вектор весов $w$, оптимизируя функционал $Q(X, w)$:\n",
    "$$Q(X, w) = \\frac{1}{l} \\sum_{i = 1}^{l} \\mathcal{L}(M_i(w)) \\rightarrow \\min_{w}$$\n",
    "\n",
    "Метод обучения, использующий логистическую функцию потерь $\\log(1 + \\exp(-M))$, называется логистической регрессией.\n",
    "Одно из основных свойств логистической регрессии — возможность корректного оценивания вероятности принадлежности объекта к каждому из классов:\n",
    "\n",
    "$$p(y = 1| x) = \\frac{1}{1 + \\exp(-\\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle))$$\n",
    "\n",
    "Нетрудно заметить, что максимизация логарифма правдоподобия выборки при такой параметризации вероятностей эквивалентна минимизации суммы логистических функций потерь. \n",
    "\n",
    "Обычно к оптимизируемому функционалу добавляют второе слагаемое — регуляризатор, не зависящий от данных. Второе слагаемое ограничивает вектор\n",
    "параметров модели тем самым уменьшая переобучение.\n",
    "Один из примеров регуляризации — $L_2$ регуляризатор:\n",
    "$$Q(X, w) = \\frac{1}{l} \\sum_{i = 1}^{l} \\mathcal{L}(M_i(w)) + \\frac{\\lambda}{2}\\Vert w \\Vert_2^2 \\rightarrow \\min_{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Многоклассовая классификация`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Пусть теперь множество $\\mathbb{Y} = \\{1, \\ldots K\\}$. Задачу многоклассовой классификации можно свести к набору бинарных задач. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### `Один против всех (one-vs-all)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Обучается $K$ классификаторов $a_1(x), \\ldots, a_K(x)$.\n",
    "Алгоритм $a_j(x)$ обучается по выборке $X_j$:\n",
    "$$X_j = (x_i, 2 \\mathbb{I}[y_i = j] - 1)_{i = 1}^{l}$$\n",
    "Таким образом, каждому классификатору $a_j(x)$ соответствует набор весов $w_j$:\n",
    "$$a_j(x) =\\mathrm{sign}\\left( \\langle w_j, x\\rangle \\right)$$\n",
    "\n",
    "Итоговый классификатор будет выдавать класс, соответствующий самому уверенному алгоритму:\n",
    "$$a(x) = \\arg \\max_{j \\in \\{1, \\ldots, k\\}}  \\langle w_j, x\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### `Каждый против каждого (all-vs-all)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Обучается $C^2_k$ классификаторов $a_{sj}(x), s, j \\in \\{1, \\ldots, k\\}, s < j$.\n",
    "Алгоритм $a_{sj}(x)$ обучается по выборке $X_{sj}$:\n",
    "$$X_{sj} = \\{(x_i, y_i) \\in X \\; | \\; y_i = s \\; \\text{или} \\; y_i = j \\}$$\n",
    "\n",
    "Таким образом, каждому классификатору $a_{sj}(x)$ соответствует набор весов $w_{sj}$:\n",
    "$$a_{sj}(x) =\\mathrm{sign}\\left( \\langle w_{sj}, x\\rangle \\right)$$\n",
    "\n",
    "Итоговый классификатор будет выдавать класс, который наберёт больше всего голосов построенных алгоритмов:\n",
    "$$a(x) = \\arg \\max_{k \\in \\{1, \\ldots, K\\}} \\sum_{s = 1}^{K} \\sum_{j \\neq s} \\mathbb{I}[a_{sj} = k]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### `Мультиномиальная регрессия`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Существует прямое обобщение логистической регрессии на случай многих классов — мультиномиальная регрессия.\n",
    "Пусть построено $K$ линейных моделей $a_1(x), \\ldots, a_K(x)$,\n",
    "$a_j(x) =\\mathrm{sign}\\left( \\langle w_j, x\\rangle \\right)$.\n",
    "Каждая модель даёт оценку принадлежности объекта к определённому классу.\n",
    "Эти оценки можно перевести в вероятности с помощью функции $\\text{Softmax}(z_1, \\ldots, z_K)$, которая переводит произвольный вещественный вектор в дискретное вероятностное распределение:\n",
    "$$\\text{Softmax}(z_1, \\ldots, z_k) = \\left( \\frac{\\exp(z_1)}{\\sum_{k = 1}^{K}\\exp(z_k)}, \\ldots, \\frac{\\exp(z_K)}{\\sum_{k = 1}^{K}\\exp(z_k)} \\right)$$\n",
    "\n",
    "Вероятность $k$-ого класса можно выразить так:\n",
    "$$P(y=j | x) =  \\frac{\\exp(\\langle w_j, x \\rangle)}{\\sum_{k = 1}^{K}\\exp(\\langle w_k, x \\rangle)}$$\n",
    "\n",
    "Обучение производится с помощью метода максимального правдоподобия:\n",
    "$$Q(X, w) = -\\frac{1}{l}\\sum_{i = 1}^{l} \\log P(y_i | x_i) + \\frac{\\lambda}{2} \\sum_{k=1}^{K}\\Vert w_k \\Vert_2^2 \\rightarrow \\min_{w_1, \\ldots, w_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Градиентный спуск`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Для минимизации функционала $Q(X, w)$ можно использовать метод градиентного спуска.\n",
    "В этом методе выбирается начальное приближение для вектора весов $w$, затем запускается итерационный процесс, на каждом шаге которого вектор $w$ изменяется в направлении антиградиента функционала $Q(X, w)$:\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\eta_k \\nabla_w Q(X, w) = w^{(k)} - \\frac{1}{l} \\eta_k \\sum_{i = 1}^{l} \\nabla_w \\mathcal{L}(M_i(w))\n",
    "$$\n",
    "\n",
    "Параметр $\\eta_k > 0$ — темп обучения (learning rate), который может равняться константе, а может, например, монотонно уменьшаться с течением итераций. В задании вам предлагается использовать формулу:\n",
    " $$\\eta_k  = \\frac{\\alpha}{k^{\\beta}}, \\quad \\text{где $\\alpha$, $\\beta$ — заданные константы}$$\n",
    " \n",
    "Остановка алгоритма может происходить при слишком малом изменении функционала или нормы вектора весов или после заданного числа итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Стохастический градиентный спуск`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Функционал $Q(X, w)$ представляет собой сумму функций потерь на каждому объекте. Вычислять градиент на каждой итерации при большом числе объектов может быть очень трудоёмко.\n",
    "Можно оценить градиент суммы градиентом одного слагаемого, на каждой итерации слагаемое выбирается случайно:\n",
    "$$\n",
    "i \\sim unif\\{l\\}\n",
    "$$\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\eta_k \\nabla_w \\mathcal{L}(M_i(w))\n",
    "$$\n",
    "\n",
    "Такой метод называется методом стохастического градиентного спуска. \n",
    "На каждой итерации можно выбирать не один объект, а небольшое подмножество (mini-batch), что ускорит сходимость алгоритма.\n",
    "Преимуществом алгоритма помимо маленькой вычислительной сложности является то, что на каждом шаге необходимо держать в память лишь один объект из обучающей выборки.\n",
    "\n",
    "**Замечание:** Обратите внимание, в отличие от Градиентного Спуска, норма градиента по минибатчу не позволяет корректным образом задать критерий останова.\n",
    "\n",
    "**Практический трюк:** Для каждой эпохи необходимо сгенерировать случайную перестановку индексов всех объектов. Далее, на каждой итерации выбирать следующее подмножество индексов. Такой трюк ускоряет работу алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Разностная проверка градиента`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "При написании собственной реализации линейной модели возникает необходимость проверить правильность её работы.\n",
    "Проверить правильность реализации подсчета градиента можно с помощью конечных разностей:\n",
    "\n",
    "$$\n",
    "[\\bigtriangledown f(w)]_i \\approx \\frac{f(x + \\varepsilon e_i) - f(x)}{\\varepsilon}\n",
    "$$\n",
    "\n",
    "$e_i = (0, 0, \\ldots, 0, 1, 0, \\ldots, 0)$ — базисный вектор, $\\varepsilon > 0$ — небольшое положительное число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Рекомендации`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Как сравнивать работу градиентного спуска и стохастического градиентного спуска?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$1$ итерация **GD** == $1$ эпоха **SGD**, поэтому сравнивать итерации некорректно! Сравнивать нужно либо по времени, либо по эпохам.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Вычисление softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:32:18.117167Z",
     "start_time": "2022-10-27T05:32:18.027919Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Замечание 1:** В промежуточных вычислениях стоит избегать вычисления значение $\\exp(-b_i \\langle x_i , w\\rangle)$, иначе может произойти переполнение. Вместо этого следует напрямую вычислять необходимые величины с помощью специализированных для этого функций: `np.logaddexp`, `scipy.special.logsumexp` и `scipy.special.expit`. В ситуации, когда вычисления экспоненты обойти не удаётся, можно воспользоваться процедурой \"клипинга\" (функция `numpy.clip`).\n",
    "    \n",
    "**Замечание 2:** При вычислении нормировки $\\frac{\\exp(\\alpha_i)}{\\sum\\limits_{k}\\exp(\\alpha_k)}$ может произойти деление на очень маленькое число, близкое к нулю. Необходимо воспользоваться следующим трюком:\n",
    "$$\\frac{\\exp(\\alpha_i)}{\\sum\\limits_{k}\\exp(\\alpha_k)} = \\frac{\\exp(\\alpha_i - \\max \\alpha_j)}{\\sum\\limits_{k}\\exp(\\alpha_k - \\max \\alpha_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Спасибо за внимание!`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Необработанный формат ячейки",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
