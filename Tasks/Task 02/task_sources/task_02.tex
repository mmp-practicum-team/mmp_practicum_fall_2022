
\documentclass[10pt,fleqn]{article}

\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{bm}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}

\textheight=26cm % высота текста
\textwidth=18cm % ширина текста
\oddsidemargin=-1cm % отступ от левого края
\topmargin=-2.5cm % отступ от верхнего края
\sloppy

\newcounter{example}

%-- Обозначение вектора жирным символом
\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}} {\mbox{\boldmath$\scriptscriptstyle#1$}}}

\DeclareMathOperator{\B}{Bin}
\DeclareMathOperator{\Ps}{Poiss}
\DeclareMathOperator{\R}{Unif}
\DeclareMathOperator{\sign}{\mathrm{sign}}
\DeclareMathOperator{\softmax}{\mathrm{softmax}}
\DeclareMathOperator{\loss}{\mathcal{L}}

\pagestyle{empty}

\begin{document}

\begin{center}
    \begin{tabular}{|p{17.5cm}|}
        \hline
        \textbf{ВМК}\\
        \begin{center} \Large Задание 2. Градиентные методы обучения линейных моделей. \\ Применение линейных моделей для определения токсичности комментария \end{center}\\
        \textbf{Практикум 317 группы, 2022}\\
        \hline
    \end{tabular}
\end{center}

\

\begin{tabbing}
    Начало выполнения задания: 29 октября 2022 года, 02:00.\\
    Мягкий Дедлайн: \textcolor{blue}{\bf 15 ноября 2022 года, 08:00.} \\
    Жёсткий Дедлайн: \textcolor{red}{\bf 22 ноября 2022 года, 08:00.} \\
\end{tabbing}

%\tableofcontents

\section*{Формулировка задания}

Данное задание направлено на ознакомление с линейными моделями и градиентными методами обучения.
В задании необходимо:
\begin{enumerate}
 \item Написать на языке Python собственную реализацию линейного классификатора с произвольной функцией потерь.
Прототипы функций должны строго соответствовать прототипам, описанным в спецификации и проходить все тесты.
Задание, не проходящее все тесты, приравнивается к невыполненному.
При написании необходимо пользоваться стандартными средствами языка Python, библиотеками \verb|numpy|, \verb|scipy| и \verb|matplotlib|. Библиотекой \verb|scikit-learn| пользоваться запрещается, если это не обговорено отдельно в пункте задания.
 \item Вывести все необходимые формулы, привести выкладки в отчёте.
 \item Провести описанные ниже эксперименты с модельными данными и приложенным датасетом.
 \item Написать отчёт о проделанной работе (формат PDF). Отчёт должен быть подготовлен в системе \LaTeX.
\end{enumerate}

\section*{Теоретическая часть}

\begin{enumerate}
\item Выведите формулу градиента функции потерь для задачи бинарной логистической регрессии. Запишите вывод.

\item Выведите формулу градиента функции потерь для задачи многоклассовой (мультиномиальной) логистической регрессии. Запишите вывод.

\item Покажите, что при количестве классов = 2, задача мультиномиальной логистической регрессии сводится к бинарной логистической регрессии.
\end{enumerate}

\section*{Список экспериментов}

Эксперименты этого задания необходимо проводить на датасете, содержащим комментарии из раздела обсуждений английской Википедии, который был преобразован для решения задачи бинарной классификации: является ли данный комментарий токсичным или нет. Подробнее об исходных данных \href{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge}{здесь}.
Требуемый для выполнения данного задания датасет можно найти по следующей \href{https://drive.google.com/open?id=1a-O_gckKyVRQPF5RhBxo-RnZBEGpezE0}{ссылке}. Данные в датасете записаны в формате csv.

\begin{enumerate}
    \item Произведите предварительную обработку текста. Приведите все тексты к нижнему регистру. Замените в тексте все символы, не являющиеся буквами и цифрами, на пробелы.

    \textbf{Замечание.}
    Полезные функции: \verb|str.lower|,  \verb|str.split|, \verb|str.isalnum|, \verb|re.sub|, \verb|re.split|.

    \item Преобразуйте выборку в разреженную матрицу \verb|scipy.sparse.csr_matrix|, где значение \verb|x| в позиции \verb|(i,j)| означает, что в документе \verb|i| слово \verb|j| встретилось \verb|x| раз.
    Разрешается воспользоваться конструктором  \verb|sklearn.feature_extraction.text.CountVectorizer|.

    \textbf{Замечание 1.} У \verb|CountVectorizer| есть несколько методов для работы, используйте \texttt{fit\_transform} и \texttt{fit} для обучающей выборки, используйте \texttt{transform} для тестовой.

    \textbf{Замечание 2.} Используйте параметр \texttt{min\_df}, чтобы уменьшить размерность данных и ускорить проведение экспериментов.

    \item Реализуйте методы градиентного и стохастического градиентного спуска в соответствии с требованиями к реализации.\ref{requirments} Сравните численный подсчет градиента функции потерь из модуля utils.py с вычислением по аналитической формуле.
    
    \item Исследуйте поведение градиентного спуска для задачи логистической регрессии в зависимости от следующих параметров:
    \begin{itemize}
        \item параметр размера шага \texttt{step\_alpha}
        \item параметр размера шага \texttt{step\_beta}
        \item начального приближения
    \end{itemize}

    Исследование поведения подразумевает анализ следуюших зависимостей:
    \begin{itemize}
        \item зависимость значения функции потерь от итерации метода (эпохи в случае стохастического варианта)

        \item зависимость точности (accuracy) итерации метода (эпохи в случае стохастического варианта)
        
        \item При желании можно проанализировать зависимость значения функции потерь и зависимость точности от реального времени работы метода и их отличия от зависимости от итерации.
    \end{itemize}

    \item Исследуйте поведение стохастического градиентного спуска для задачи логистической регрессии в зависимости от следующих параметров:
    \begin{itemize}
        \item параметр размера шага \texttt{step\_alpha}
        \item параметр размера шага \texttt{step\_beta}
        \item размер подвыборки \texttt{batch\_size}
        \item начального приближения
    \end{itemize}


    \item Сравните поведение двух методов между собой, сделайте выводы.

    \item Примените алгоритм лемматизации (например,  \texttt{WordNetLemmatizer} из библотеки \texttt{nltk}) к коллекции. Удалите из текста стоп-слова (например, используя список стоп-слов из \texttt{nltk}). Исследуйте, как предобработка корпуса повлияла на точность классификации, время работы алгоритма и размерность признакового пространства.

    \item Исследуйте качество, время работы алгоритма и размер признакового пространства в зависимости от следуюших факторов:

    \begin{itemize}
        \item использовалось представление \verb|BagOfWords| или \verb|Tfidf|
        \item параметров \texttt{min\_df} и \texttt{max\_df} конструкторов.
    \end{itemize}

    \textbf{Замечание.} Для построения tf-idf представления воспользуйтесь \texttt{TfidfTransformer} или \texttt{TfidfVectorizer} из библиотеки \texttt{sklearn}.

    \item Выберите лучший алгоритм для тестовой выборки. Проанализируйте ошибки алгоритма. Проанализируйте и укажите общие черты объектов, на которых были допущены ошибки.

\end{enumerate}


\section*{Требования к реализации}
\label{requirments}
%\textbf{Замечание 1.} Везде выборкой объектов будем понимать \texttt{numpy array} размера $N \times D$ или разреженную матрицу \texttt{scipy.sparse.csr\_matrix} того же размера,
%под ответами для объектов выборки будем понимать \texttt{numpy array} размера $N$, где $N$ --- количество объектов в выборке, $D$ --- размер признакового пространства.
%\\

Прототипы всех функций описаны в файлах, прилагающихся к заданию.

Среди предоставленных файлов должны быть следующие модули и функции в них:

\begin{enumerate}

\item Модуль \texttt{oracles.py} с реализациями функций потерь и их градиентов.

 Обратите внимание на то, что все функции должны быть полностью векторизованы (т.е. в них должны отсутствовать циклы).

\textbf{Замечание 1.} В промежуточных вычислениях стоит избегать вычисления $\exp(-b_i \langle x_i , w\rangle)$, иначе может произойти переполнение. Вместо этого следует напрямую вычислять необходимые величины с помощью специализированных для этого функций: \texttt{np.logaddexp}, \texttt{scipy.special.logsumexp} и \texttt{scipy.special.expit}.
В ситуации, когда вычисления экспоненты обойти не удаётся, можно воспользоваться процедурой <<клипинга>> (функция \texttt{numpy.clip}).

\textbf{Замечание 2.} При вычислении нормировки $\frac{\exp(\alpha_i)}{\sum_{k}\exp(\alpha_k)}$ может произойти деление на очень маленькое число, близкое к нулю. Необходимо воспользоваться следующим трюком:
$$
\frac{\exp(\alpha_i)}{\sum_{k}\exp(\alpha_k)} = \frac{\exp(\alpha_i - \max \alpha_j)}{\sum_{k}\exp(\alpha_k - \max \alpha_j)}
$$

\item Модуль \texttt{utils.py} с реализацией функции численного подсчёта градиента произвольного функционала.

\item Модуль \texttt{optimization.py} с реализацией методов обыкновенного и стохастического градиентного спуска.

\end{enumerate}

\textbf{Замечание.} Для всех функций можно задать аргументы по умолчанию, которые будут удобны вам в вашем эксперименте.
Ко всем функция можно добавлять необязательные аргументы, а в словарь \texttt{history} разрешается сохранять необходимую в ваших экспериментах информацию.

\section*{Бонусная часть}

\begin{enumerate}
\item \textbf{(до 2 баллов)} Добавьте в признаковое пространство $n$-граммы (измените параметр \texttt{ngramm\_range} у \texttt{TfidfTransformer}). Исследуйте, как влияет размер максимальных добавленных $n$-грамм на качество и скорость работы алгоритма.

\item \textbf{(до 5 баллов)} Реализуйте режим работы алгоритма \texttt{SGDClassifier}, при котором вся обучающая выборка не будет храниться в оперативной памяти.

\begin{itemize}
    \item Перемешайте документы в исходном файле, чтобы большое количество документов одного класса не шли подряд друг за другом.
    \item Реализуйте итератор, который будет считывать следующие \texttt{batch\_size} строк из созданного файла и преобразовывать их в \texttt{numpy.array} или \texttt{sparse.csr\_matrix} соответствующего размера. При считывании последнего документа, итератор начинает считывание документов заново.
    \item Реализуйте специальный режим обучения для алгоритма стохастического градиентного спуска, который будет принимать на вход вместо матрицы объекты-признаки рассмотренный выше итератор.
\end{itemize}

Попробуйте оценить сколько памяти удаётся сэкономить, используя такой подход, и насколько медленнее такой подход по времени.

Алгоритм, реализованный таким образом, на каждой итерации не будет выбирать произвольные документы, а будет брать следующую пачку документов, пришедшую из итератора. Таким образом, алгоритм станет <<менее случаен>>. Исследуйте, повлияет ли это на качество работы алгоритма. Рассмотрите несколько различных значений параметра \texttt{batch\_size}.

\textbf{Замечание.} Критерий остановки оптимизации алгоритма связанный с разностью значений функции на соседних эпохах для такого режима обучения неэффективен, необходимо использовать другие критерии, не зависящие от объектов всей выборки, либо производить константное число итераций метода.

\item \textbf{(до 5 баллов)} Улучшить качество работы линейных алгоритмов на датасете с помощью средств, не использующихся в задании. Например, можно реализовать ансамбль линейных алгоритмов, использовать продвинутые методы выделения коллокаций, выделять специальные термины или сущности в тексте. Обратите также внимание на специфику задачи: вполне возможно, что альтернативная обработка текста (учет регистра, знаков препинания и т.п.) поможет значительно улучшить качество вашей модели. Размер бонуса зависит от величины улучшения и от изобретательности подхода.

\end{enumerate}

\end{document}
